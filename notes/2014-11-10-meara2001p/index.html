<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>Notes: Meara - 2001 - P-Lex: A Simple and Effective Way of Describing the lexical Characteristics of Short L2 Tests. - Bodong Chen</title>
    <meta property="og:title" content="Notes: Meara - 2001 - P-Lex: A Simple and Effective Way of Describing the lexical Characteristics of Short L2 Tests. - Bodong Chen">
    

    
      
    

    

    
    

    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    
<link rel="stylesheet" href="/css/custom.css" />

  </head>

  
  <body class="notes">
    <header class="masthead">
      <h1><a href="/">Bodong Chen</a></h1>

<p class="tagline">Crisscross Landscapes</p>

      <nav class="menu">
        <input id="menu-check" type="checkbox" />
        <label id="menu-label" for="menu-check" class="unselectable">
          <span class="icon close-icon">✕</span>
          <span class="icon open-icon">☰</span>
          <span class="text">Menu</span>
        </label>
        <ul>
        
        
        <li><a href="/">Home</a></li>
        
        <li><a href="/blog/">Blog</a></li>
        
        <li><a href="/cv/">Publications</a></li>
        
        <li><a href="/notes/">Open Scholar</a></li>
        
        <li><a href="/teaching/">Teaching</a></li>
        
        <li><a href="/%E5%8D%9A%E5%AE%A2/">博客</a></li>
        
        <li><a href="/index.xml">Subscribe</a></li>
        
        
        </ul>
      </nav>
    </header>

    <article class="main">
      <header class="title">
      
<h1>Notes: Meara - 2001 - P-Lex: A Simple and Effective Way of Describing the lexical Characteristics of Short L2 Tests.</h1>

<h3>
</h3>
<hr>


      </header>





<h2 id="references">References</h2>

<p><strong>Citekey</strong>: @meara2001p</p>

<p>Meara P and Bell H (2001). “P-Lex: A Simple and Effective Way of
Describing the lexical Characteristics of Short L2 Tests.” <em>Prospect</em>,
<em>16</em>(3), pp. 5-19.</p>

<h2 id="notes">Notes</h2>

<p>This article introduces a novel way to assess productive vocabulary &ndash; P-LeX &ndash; which is better at handling shorter text (in comparison with LFP) and easier to interpret.</p>

<h2 id="highlights">Highlights</h2>

<p>ABSTRACT This paper describes an alternative approach to assessing the lexical complexity of short texts produced by second language learners of English. This methodology bears a passing resemblance to the Lexical Frequency Profile (LFP) suggested by Laufer and Nation (1995), but the approach it takes is mathematically more sophisticated, and the data it produces is easier to work with. We argue that P_Lex produces data which is broadly comparable with the data produced by LFP. However, P_Lex works much better with shorter texts than LFP does, and this makes it a better tool for evaluating texts produced by low-level learners. (p. 5)</p>

<p>The most common approach to this problem has been to work with measures of lexical richness, or lexical diversity. Indices of this sort have been widely used in studies of lexico-statistics (eg Herdan 1960) and L1 development (eg Miller and Klee 1995), and they have begun to appear with increasing frequency in work on L2 speakers (eg Arnaud 1984, Broeder et al 1988, Malvern and Richards 1997). (p. 5)</p>

<p>Most of this work uses measures that compare the number (p. 5)</p>

<p>of Lexical Types in a text with the number of Lexical Tokens in the same text. A number of measures of this sort exist (see Table 1), but there is no clear agreement about which is the best variant to use in the context of L2 learners. (p. 6)</p>

<p>The main practical difficulty with measures based on Types and Tokens is that they are sensitive to the length of the text being assessed (longer texts typically have lower Type/Token Ratios than shorter ones do) (p. 6)</p>

<p>The measures listed in Table 1 are all examples of what we might call Intrinsic Measures of Lexical Variety. In these measures, variety is assessed solely in terms of the words that appear in the text itself. (p. 6)</p>

<p>This suggests that there might be a case for developing some Extrinsic Measures of Lexical Richness for use with L2 learners. These measures would not be limited to the number of Types or Tokens appearing in an L2 text: they would supplement this information with additional information about the sorts of words being used, and the sorts of lexical choices that are being made in a particular text. (p. 7)</p>

<p>An example of a measure of this sort is to be found in Laufer and Nation’s Lexical Frequency Profile (Laufer and Nation 1995). (p. 7)</p>

<p>In our experience, LFP has poor measurement characteristics and does not discriminate well between texts, because it relies very heavily on a simple count of the number of Category Three and Category Four words in the text. The number of these words in a ‘typical’ text is usually very small, and this severely limits the way LFP works. (p. 8)</p>

<p>More importantly, a serious practical problem with LFP is that it requires relatively long texts for stable measures to emerge. Laufer and Nation claim that in their data ‘profiles over 200 words were found to be stable, while those done on less than 200 words were not’ (1995: 314). (p. 9)</p>

<p>P_Lex (p. 9)</p>

<p>P_Lex is based on the idea that it might be possible to make a virtue out of the fact that ‘difficult’ words occur only infrequently in texts. P_Lex looks at the distribution of difficult words in a text, and returns a simple index that tells us how likely the occurrence of these words is. The underlying assumption here is that people with big vocabularies are more likely to use infrequent words than people with smaller vocabularies are, and that we can use the index we derive from the texts as a pointer to vocabulary size. (p. 9)</p>

<p>Not surprisingly, it turns out that the distributions we get for this type of analysis are strongly skewed to the left: most texts contain few difficult words, and texts that contain a very high proportion of such words are themselves quite unusual. Distributions that are strongly skewed to the left are often (p. 10)</p>

<p>Poisson distributions (p. 11)</p>

<p>The mathematics of fitting curves to data is fairly complex, so we have summarised this process in detail in Appendix A. For readers who don’t want to get that involved, it is enough to know that there is a procedure which makes it possible to turn data like that in Table 3 into a single figure, conventionally known as lambda. (p. 11)</p>

<p>Lambda values typically range from 0 to about 4.5, with higher figures corresponding to a higher proportion of infrequent words. Lambda values have good measurement characteristics, and this allows them to be added and averaged straightforwardly. (p. 11)</p>

<p>More importantly, however, lambda scores are much less sensitive to text length than the LFP scores are, and, critically, the P_Lex methodology gives lambda scores that are reasonably stable with very short texts. (p. 11)</p>

<p>An evaluation of P_Lex This section illustrates the way P_Lex works with a large set of texts produced by L2 learners of English. (p. 11)</p>

<p>Our basic question was whether the P_Lex methodology is reliably stable across administrations (p. 12)</p>

<p>We also examined whether the P_Lex measure was able to distinguish reliably between groups of learners at different levels of proficiency. (p. 13)</p>

<p>However, it is clear from Figure 2 that P_Lex is essentially stable from about 120 words, and that texts of this length clearly discriminate between the proficiency levels (p. 13)</p>

<p>illustrated. (p. 14)</p>

<p>The data in this figure suggest that it might be possible to get reliable P_Lex data from texts that were considerably shorter than the texts analysed in the main experiment. (p. 14)</p>

<p>Discussion (p. 15)</p>

<p>The data reported above suggest that the P_Lex methodology is basically a reliable one, which produces data very similar to the data produced by LFP. However, P_Lex has the advantage that it seems to work with much shorter texts than the recommended minimum test length for LFP, and this makes it a more useful tool for analysing the output of L2 learners, particularly lowerlevel learners. (p. 15)</p>

<p>The question of validity is much more awkward to deal with. There are two basic problems here. The first problem is that there are no other tests of productive vocabulary with which we can compare these data. (p. 15)</p>

<p>Our approach here has been to use the so-called Productive Version of the Levels Test as a comparison point, but there are a number of reasons for viewing the Levels Test as a poor instrument (p. 15)</p>

<p>The second problem concerns our selection of ‘difficult’ words. In the work reported here, we have defined ‘difficult’ in terms of frequency, a practice that is largely unquestioned in this field. The version of P_Lex used here used Nation’s (1984) word lists as a way of discriminating between ‘easy’ words and ‘hard’ words. We arbitrarily assigned words in Nation’s 1000 word list to the former category, along with proper nouns, numerals and geographical derivatives, while any other words were assigned to the latter category. We think, however, that there might be a case for exploring alternative ways of characterising vocabulary. (p. 15)</p>

<p>Specifically, we think that ‘difficult’ vocabulary is not entirely to be defined in terms of frequency: words are unusual in particular contexts for particular groups of L1 speakers, and it may not be possible to draw up a list of ‘difficult’ (p. 15)</p>

<p>Other ‘difficult’ words would then actually indicate a lack of appropriate vocabulary. (p. 16)</p>

<p>Appendix A: How the lambda scores are calculated</p>

<p>The advantage of fitting Poisson curves to our data is that these curves are conveniently described by a compact formula:</p>

<p>PN =(λN *e-λ )/N!</p>

<p>The critical value in this formula is the variable λ (lambda), which defines the overall shape of the curve. If we know the value of lambda, then we know what the curve will look like, and this means that we can use lambda as a short- hand for describing data like the sample presented in Table 3.</p>

<p>Inevitably, these curves are not exact fits, and P_Lex reports an Error Figure, which shows how well the data are described by the best-fitting Poisson curve.</p>


  <footer>
  
<nav class="post-nav">
  <span class="nav-prev">&larr; <a href="/notes/2016-07-14-michalak2016/">Notes: Michalak, R. (2016). Diigo</a></span>
  <span class="nav-next"><a href="/notes/2014-11-09-meara2000lex30/">Notes: Meara - 2000 - Lex30: an improved method of assessing productive vocabulary in an L2</a> &rarr;</span>
</nav>
<script type="text/javascript">
document.addEventListener('keyup', function(e) {
  if (e.target.nodeName.toUpperCase() != 'BODY') return;
  var url = false;
  if (e.which == 37) {  
    
    url = '\/notes\/2016-07-14-michalak2016\/';
    
  } else if (e.which == 39) {  
    
    url = '\/notes\/2014-11-09-meara2000lex30\/';
    
  }
  if (url) window.location = url;
});
</script>



<section class="comments">
  <div id="disqus_thread"></div>
  <script src="/js/disqusloader.min.js"></script>
  <script>
  var disqus_config = function () {
  
  };
  (function() {
    var inIFrame = function() {
      var iframe = true;
      try { iframe = window.self !== window.top; } catch (e) {}
      return iframe;
    };
    if (inIFrame()) return;
    var disqus_js = '//meefen.disqus.com/embed.js';
    
    if (location.hash.match(/^#comment/)) {
      var d = document, s = d.createElement('script');
      s.src = disqus_js; s.async = true;
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    } else {
      disqusLoader('#disqus_thread', {
        scriptUrl: disqus_js, laziness: 0, disqusConfig: disqus_config
      });
    }
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>







  

  
  <hr>
  <div class="copyright">&copy; <a href="http://meefen.github.io/">Bodong Chen</a> 2015-2018 | <a href="https://github.com/meefen">Github</a> | <a href="https://twitter.com/bod0ng">Twitter</a></div>
  
  </footer>
  </article>
  
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-50108133-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

  </body>
</html>

